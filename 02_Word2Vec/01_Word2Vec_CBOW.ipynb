{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론(예측) 기반 기법과 신경망\n",
    "\n",
    "## word2vec : 워드투벡터\n",
    "2013년 구글의 토마스미콜로프(Tomas Mikolov)의 팀이 개발<br>\n",
    "<b>word2vec</b> 알고리즘은 <b>신경망 모델</b>을 사용 하여 큰 텍스트 코퍼스에서 단어 연관성을 학습. 학습이 끝나면 이러한 모델은 동의어 단어를 감지하거나 부분 문장에 대한 추가 단어를 제안 할 수 있다. word2vec는 <b>벡터</b> 라고하는 특정 숫자 목록을 사용하여 각각의 고유 한 단어를 나타낸다 . 벡터는 간단한 수학적 함수 ( 벡터 간의 코사인 유사성 ) 가 해당 벡터가 나타내는 단어 간의 의미 유사성 수준을 나타내 도록 신중하게 선택 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 신경망에서의 단어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      " [[1 0 0 0 0 0 0]]\n",
      "W:\n",
      " [[ 0.61286584 -1.2466989  -1.3517066 ]\n",
      " [ 0.52619045 -1.94202858 -1.21317494]\n",
      " [-1.26534331 -2.58442609  0.41095552]\n",
      " [-0.34906414  0.40886373 -0.7579579 ]\n",
      " [ 0.37512806 -1.84793732 -0.05202047]\n",
      " [ 0.20910327 -0.3389521   0.27308666]\n",
      " [ 0.64394741  0.1789175  -0.68014251]]\n",
      "h:\n",
      " [[ 0.61286584 -1.2466989  -1.3517066 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
    "\n",
    "# 여기서 'you'만  one-hot 인코딩으로 표현\n",
    "c = np.array([[1,0,0,0,0,0,0]])   # (1,7)\n",
    "print('c:\\n',c)\n",
    "\n",
    "W = np.random.randn(7,3)\n",
    "print('W:\\n',W)\n",
    "\n",
    "h = np.matmul(c,W)    # (1,7) * (7,3) = (1,3)\n",
    "print('h:\\n',h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 단순한 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW (Continuous Bag of Words) 모델\n",
    "\n",
    "#### Word2Vec에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있다\n",
    "- $ CBOW $ 는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법 <br>\n",
    "  타깃(target)은 중앙 단어 그 주변 단어들이 맥락(contexts)이다\n",
    "- $ Skip-Gram $ 은 중간에 있는 단어로 주변 단어들을 예측하는 방법\n",
    "\n",
    "#### BOW(Bag of Words) : 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "\n",
    "BOW를 만드는 과정<br>\n",
    "(1) 우선, 각 단어의 고유한 인덱스(Index)를 부여한다.<br>\n",
    "(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터(Vector)를 만든다.<br>\n",
    "\n",
    "\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"<br>\n",
    "('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9) <br>\n",
    "BOW: [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]  ==> '가' 와 '물가상승률' 은 2회 발생\n",
    "\n",
    "https://wikidocs.net/22650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 준비\n",
    "#### 맥락과 타깃을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynlp import preprocess, create_contexts_target,convert_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)      # 8개\n",
    "print(id_to_word)  # 7개\n",
    "print(word_to_id)  # 7개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]] (6, 2)\n"
     ]
    }
   ],
   "source": [
    "contexts,target = create_contexts_target(corpus,window_size=1)\n",
    "print(contexts,contexts.shape)\n",
    "# 맥락(contexts) : 예측할 단어의 주변 단어\n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
    "# window_size=1 일 경우 : 주변 단어를 중간 단어에 앞,뒤로 1개만 사용\n",
    "# [[0 2]   : 'you', 'goodbye'\n",
    "#  [1 3]   : 'say', 'and'\n",
    "#  [2 4]   : 'goodbye', 'i'\n",
    "#  [3 1]   : 'and', 'say'\n",
    "#  [4 5]   : 'i', 'hello'\n",
    "#  [1 6]]  : 'say', '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "print(target)   # (6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]] (6, 7)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "print(vocab_size)\n",
    "target = convert_one_hot(target,vocab_size)\n",
    "print(target,target.shape)  # (6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]] (6, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "contexts = convert_one_hot(contexts,vocab_size)\n",
    "print(contexts,contexts.shape)  # (6,2,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 신경망 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import MatMul,SoftmaxWithLoss,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self,vocab_size,hidden_size):  # 어휘수 : 7개, 은닉층의 뉴런 : 5\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01*np.random.randn(V,H).astype('f')  # (7,5)\n",
    "        W_out = 0.01*np.random.randn(H,V).astype('f') # (5,7)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다\n",
    "        layers = [self.in_layer0,self.in_layer1,self.out_layer]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers: # 3회\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "       \n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vec = W_in           \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
